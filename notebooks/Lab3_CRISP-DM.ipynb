{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Schwan, Stuart Miller, Justin Howard, Paul Adams\n",
    "# Lab Three: Clustering, Association Rules, or Recommenders\n",
    "## Capstone: Association Rule Mining, Clustering, or Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab3 Project Requirments -\n",
    "1. [Business understanding](#Businessunderstanding)\n",
    "    1. [Describe the purpose of the data set you selected](#Assessthecurrentsituation)\n",
    "    2. [Describe how you would define and measure the outcomes from the dataset](#CostBenefit)\n",
    "    3. [How would you measure the effectiveness of a good prediction algorithm](#Desiredoutputs)\n",
    "  \n",
    "2. [Data Understanding](#Dataunderstanding)\n",
    "    1. [Describe the meaning and type of data for each attribute in the data file](#Describedata)\n",
    "    2. [Verify data quality: Explain any missing values, duplicate data, and outliers](#Datareport)\n",
    "    3. [Give simple, appropriate statistics (range, mode, mean, median, variance, counts, etc.) for the most important attributes and describe what they mean](#Stats)\n",
    "    4. [Visualize the most important attributes appropriately (at least 5 attributes)](#Distributions)\n",
    "    5. [Explore relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate](#Correlations)\n",
    "    6. [Identify and explain interesting relationships between features and the class you are trying to predict](#relationships)\n",
    "    7. [Are there other features that could be added to the data or created from existing features? Which ones?](#Featurecreation)\n",
    "    8. [Outlier Removal](#OutlierRemoval)\n",
    "\n",
    "3. [Modeling and Evaluation](#Model)\n",
    "    1. [Option A: KMeans Cluster Analysis](#KMeans)\n",
    "    2. [Option B: t-SNE Cluster Analysis](#t-SNE)\n",
    "    \n",
    "4. [Deployment](#Deployment)\n",
    "    1. [How useful is your model for interested parties? ](#Useful)\n",
    "    2. [How would you measure the model's value if it was used by these parties?](#Value)\n",
    "    3. [How would your deploy your model for interested parties?](#Deploy)\n",
    "    4. [How often would your model need to be updated?](#Update)\n",
    "    5. [What other data should be collected? ](#Collect)\n",
    "\n",
    "A1. [Model Hyperparameter Tuning Details](#A2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project will follow a hybrid methodology, mixing the expectations of the grading rubric with the CRISP_DM framework. CRISP-DM stands for the cross-industry process for data mining, which provides a structured approach to planning a data mining project. It is a robust and well-proven methodology.\n",
    "\n",
    "We are continuing the data cleaning and preperation loop for preparing the dataset for cluster analysis.\n",
    "\n",
    "In the final we have choosen to test different models for clustering of our dataset. The two approaches we have taken are\n",
    "* [Model A: KMeans Clustering](#KMeans)\n",
    "* [Model B: t-SNE Clustering](#t-SNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../_images/crisps-dm3.png\" style=\"width:550px;height:450px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Stage One - Determine Business Objectives and Assess the Situation  <a class=\"anchor\" id=\"Businessunderstanding\"></a>\n",
    "We will use the Home Credit Default Risk dataset made available on Kaggle to develop a useful model that predicts loan defaults for a majority of the loan applicants whose population is defined by the given training and test datasets. Predicting loan defaults is essential to the profitability of banks and, given the competitive nature of the loan market, a bank that collects the right data can offer and service more loans. This analysis of Home Credit's Default Risk dataset will focus on generating accurate loan default risk probabilities, identifying sub-populations among the given applicants, and finally, the most critical factors that indicate that an applicant will likely default on their loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Assess the Current Situation (Q1A)<a class=\"anchor\" id=\"Assessthecurrentsituation\"></a>\n",
    "Home Credit is an international non-bank financial institution that operates in 10 countries and focuses on lending to people with little or no credit history. This institution has served 11 million customers, is based in the Czechia, and is a significant consumer lender in most of the Commonwealth of Independent States Countries, especially Russia. Recently, it has established a presence in China and the United States. The dataset provided is extensive, representing 307,511 applications from various locations. \n",
    "\n",
    "The data types vary in scale and type, from time-series credit histories to demographic indicators. Our analysis will focus on two datasets, data collected in the application train and test datasets, and several engineered features gathered from the millions of credit bureau records for each loan applicant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Measuring the effectiveness of a good algorithm- <a class=\"anchor\" id=\"Requirements\"></a> \n",
    "#### 1. Effective Clustering Metric: Cluster Validity\n",
    "\n",
    "The dataset contains 326 attributes for 307,511 loan applicants. An algorithm that clusters the dataset with a high degree of cluster validity will be difficult to obtain due to the high space-time complexity of clustering algorithms at our disposal. To reduce the space-time complexity of the dataset, we have elected to perform K-Means clustering on the first two principal components.\n",
    "\n",
    "- **Business success criteria**\n",
    "\n",
    "- **Data mining success criteria**\n",
    "\n",
    "#### 2. Effective Association Rule Determination\n",
    "\n",
    "- **Business success criteria**\n",
    "\n",
    "- **Data mining success criteria**\n",
    "\n",
    "#### 3. Effective Collaborative Filtering\n",
    "\n",
    "- **Business success criteria**\n",
    "\n",
    "- **Data mining success criteria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stage  Two - Data Understanding <a class=\"anchor\" id=\"Dataunderstanding\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initial Data Report (Q2) <a class=\"anchor\" id=\"Datareport\"></a>\n",
    "Our data comes from the [Kaggle Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk/overview) competition website. \n",
    "\n",
    "Our analysis features the use of several Python libraries, such as Pandas, in addition to a custom data cleaning script for both the `application` and `bureau` datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries Required.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# import custom code\n",
    "from project_code.cleaning import read_clean_data, missing_values_table, load_bureau, create_newFeatures, fill_occupation_type\n",
    "from project_code.tables import count_values_table\n",
    "\n",
    "# some defaults\n",
    "pd_max_rows_default = 60\n",
    "\n",
    "#removing warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "# path =  './application_train.csv'\n",
    "# note that XNA is a encoding for NA interpret as np.nan\n",
    "df =  pd.read_csv('./application_train.csv', na_values = ['XNA'])\n",
    "#loading bureau dataset\n",
    "\n",
    "bureau = pd.read_csv('./bureau.csv', na_values = ['XNA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state = random_state).fit(std_df)\n",
    "exp_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Standardized Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid inserting random noise from the principal component analysis, we will use the minimum number of Principal Components to arrive at a cumulative variance of 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(exp_var >= .99)\n",
    "print('Optimal Number of Principal Components: ', idx[0][0])\n",
    "X_pca = np.array(X_pca.loc[:,:87])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Verify data quality: Explain any missing values, duplicate data, and outliers (Q2B) <a class=\"anchor\" id=\"Datareport\"></a>\n",
    "We will use two of the files from the total dataset.\n",
    "\n",
    "application_train.csv: Information provided with each loan application\n",
    "bureau.csv: Information regarding clients from the credit bureaus\n",
    "The two data files can be joined on the loan id (SK_ID_CURR).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Give simple, appropriate statistics (range, mode, mean, median, variance, counts, etc.) for the most important attributes and describe what they mean (Q2C) <a class=\"anchor\" id=\"Stats\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Script\n",
    "\n",
    "All the cleaning discussed in the sections above are implemented in `cleaning.py`.\n",
    "This script contains a function (`read_clean_data`) to apply the cleaning steps and return the cleaned dataset for work.\n",
    "\n",
    "**Details**  \n",
    "* Cleaning\n",
    "  * Read csv with Pandas (setting correct data types)\n",
    "  * Drop columns that will not be used\n",
    "  * Recode NA values that are not listed as np.nan\n",
    "  * Formattings\n",
    "  * Encode categorical variables\n",
    "* Returns\n",
    "  * DataFrame with cleaned data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Visualize the most important attributes appropriately (Q2D) <a class=\"anchor\" id=\"Visualize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info\n",
    "\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Distributions  <a class=\"anchor\" id=\"Distributions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the exploration focused on the use of box plots and histograms for visualing continuous variables and bar charts for visualizing categorical variables. These graphical formats permit the easy identification of skewedness and help us identify outliers. \n",
    "\n",
    "Variables we expect to be important were selected for univariate visualization, such as `AMT_INCOME_TOTAL` and `AMT_ANNUITY`. The histogram shows the features of the main distribution, while the behavior of the tails and extreme values are shown in the boxplots. The distribution of incomes is extremely long-tailed and right-skewed, as can be expected with most any income distribution. \n",
    "\n",
    "Boxplots indicate a large number of outliers in both the `AMT_TOTAL_INCOME` and `AMT_ANNUITY` feature and one extreme outlier that will require closer exmaination. A comparison of each distribution in its raw and transformed form is displayed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Explore relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate (Q2E) <a class=\"anchor\" id=\"Correlations\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Identify and explain interesting relationships between features and the class you are trying to predict (Q2F) <a class=\"anchor\" id=\"relationships\"></a>\n",
    "\n",
    "infor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Are there other features that could be added to the data or created from existing features (Q2G) <a class=\"anchor\" id=\"Featurecreation\"></a>\n",
    "Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stage  Three - Modeling and Evaluation <a class=\"anchor\" id=\"Model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Option A: Cluster Analysis (Q3A)<a class=\"anchor\" id=\"Cluster\"></a>\n",
    "\n",
    "**Clustering using K-Means**<a class=\"anchor\" id=\"KMeans\"></a>\n",
    "\n",
    "A dataset with 307,511 records restricts the tools we can use to arrive at useful clusters of data without losing very large numbers of data. We elected to use the K-Means method using the first two principal component values for each record to drastically reduce the dimensionality of the dataset and minimize the information that is lost in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing dataset to 2 PCs\n",
    "X_pca2 = X_pca[:,:2]\n",
    "# conducting a 2 cluster KMeans operation to create benchmark visualization\n",
    "km = KMeans(n_clusters = 2,\n",
    "            init = 'random',\n",
    "            n_init = 10,\n",
    "            max_iter = 300,\n",
    "            tol = .0001,\n",
    "           random_state = random_state,\n",
    "           n_jobs = -1)\n",
    "y_km = km.fit_predict(X_pca2)\n",
    "\n",
    "markers = ('s','x', 'o', '^', 'v')\n",
    "colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "plt.figure(figsize = (14,7))\n",
    "plt.subplot(1,2,1)\n",
    "for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x = X_pca2[y.TARGET == cl, 0],\n",
    "                    y = X_pca2[y.TARGET == cl, 1],\n",
    "                    alpha = .4,\n",
    "                    color = cmap(idx),\n",
    "                    edgecolor = 'black',\n",
    "                    marker = markers[idx],\n",
    "                    label =cl)\n",
    "plt.title('PCA of application_train Dataset')\n",
    "plt.legend(scatterpoints = 1, loc = 'lower right')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_pca2[y_km == 0, 0],\n",
    "           X_pca2[y_km == 0 , 1],\n",
    "           s = 50, c = 'lightgreen',\n",
    "           marker = 's', edgecolor = 'black', \n",
    "           label = 'Cluster 1')\n",
    "\n",
    "plt.scatter(X_pca2[y_km == 1, 0],\n",
    "           X_pca2[y_km == 1, 1],\n",
    "           s = 50, c = 'orange',\n",
    "           marker = 'o', edgecolor = 'black', \n",
    "           label = 'Cluster 2')\n",
    "plt.scatter(km.cluster_centers_[:,0],\n",
    "           km.cluster_centers_[:,1],\n",
    "           s = 250,\n",
    "           marker = '*',\n",
    "            c = 'red',\n",
    "           label = 'Centroids')\n",
    "plt.legend(scatterpoints = 1, loc = 'lower right')\n",
    "plt.title('K-Means with 2 Clusters')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Significant Findings**\n",
    "\n",
    "It was clear from the initial K-Means clustering, that a more ideal number of clusters is available, so an elbow plot was created by using the between_cluster sum of squared errors for a range of clusters from 1 - 12 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSEs = []\n",
    "\n",
    "for i in range(1,13):\n",
    "    km = KMeans(n_clusters = i,\n",
    "               init = 'k-means++',\n",
    "               n_init= 10,\n",
    "               max_iter= 300,\n",
    "               random_state = random_state,\n",
    "               n_jobs = -1)\n",
    "    km.fit(X_pca2)\n",
    "    SSEs.append(km.inertia_)\n",
    "\n",
    "\n",
    "   \n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(range(1,13), SSEs, marker = 'o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster SSE')\n",
    "plt.tight_layout()\n",
    "plt.title('Elbow Plot: SSE Reduction by Cluster Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Significant Findings**\n",
    "\n",
    "We elect to describe the data using 5 clusters to create new features to help describe the clusters of applicants. To begin our decription of the 5 chosen clusters, we will first visualize the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 5,\n",
    "            init = 'k-means++',\n",
    "            n_init = 10,\n",
    "            max_iter = 300,\n",
    "            tol = .0001,\n",
    "           random_state = 1)\n",
    "y_km = km.fit_predict(X_pca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.scatter(X_pca2[y_km == 0, 0],\n",
    "           X_pca2[y_km == 0 , 1],\n",
    "           s = 30, c = 'lightgreen',\n",
    "           marker = 's', edgecolor = 'black', \n",
    "           label = 'Cluster 1')\n",
    "\n",
    "plt.scatter(X_pca2[y_km == 1, 0],\n",
    "           X_pca2[y_km == 1, 1],\n",
    "           s = 30, c = 'orange',\n",
    "           marker = 'o', edgecolor = 'black', \n",
    "           label = 'Cluster 2')\n",
    "\n",
    "plt.scatter(X_pca2[y_km == 2, 0],\n",
    "           X_pca2[y_km == 2, 1],\n",
    "           s = 50, c = 'yellow',\n",
    "           marker = '*', edgecolor = 'black', \n",
    "           label = 'Cluster 3') \n",
    "plt.scatter(X_pca2[y_km == 3, 0],\n",
    "           X_pca2[y_km == 3, 1],\n",
    "           s = 50, c = 'lightblue',\n",
    "           marker = '^', edgecolor = 'black', \n",
    "           label = 'Cluster 4') \n",
    "plt.scatter(X_pca2[y_km == 4, 0],\n",
    "           X_pca2[y_km == 4, 1],\n",
    "           s = 40, c = 'pink',\n",
    "           marker = 'd', edgecolor = 'black', \n",
    "           label = 'Cluster 5') \n",
    "plt.scatter(km.cluster_centers_[:,0],\n",
    "           km.cluster_centers_[:,1],\n",
    "           s = 250,\n",
    "           marker = '*',\n",
    "            c = 'red',\n",
    "           label = 'Centroids')\n",
    "\n",
    "plt.legend(scatterpoints = 1, loc = 'lower right')\n",
    "plt.title('K-Means: 5 Clusters')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_1 = np.array([n for n  in y_km if n == 0])\n",
    "c_2 = np.array([n for n  in y_km if n == 1])\n",
    "c_3 = np.array([n for n  in y_km if n == 2])\n",
    "c_4 = np.array([n for n  in y_km if n == 3])\n",
    "c_5 = np.array([n for n  in y_km if n == 4])\n",
    "\n",
    "print(c_1.shape,\n",
    "      c_2.shape,\n",
    "      c_3.shape,\n",
    "      c_4.shape,\n",
    "      c_5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Significant Findings **\n",
    "\n",
    "We see that our 5 clusters vary in their density and magnitude. These clusters were formed based on the Euclidean Distance of the first two Principal Components of the standardized dataset. \n",
    "\n",
    "This makes it nearly impossible to interpret the characteristics of the 5 clusters we have identified. \n",
    "To clarify what makes these clusters different, we can add a cluster feature to the raw dataset and conducting a Linear Discriminate Analysis (LDA). Since this analysis is purely descriptive, a finely tuned model is not deemed necessary. \n",
    "\n",
    "To stay aligned with the assumptions of LDA, log transformations will be applied to all predictor variables, excluding the `EXT_SOURCE` variables and the binary indicator variables. We chose to exclude the `EXT_SOURCE` variables from trnasformations because they are already transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing dataset size to 20% to reduce the computational time of silhouette sampling\n",
    "tiny_pca = pd.DataFrame(X_pca2)\n",
    "tiny_pca = tiny_pca.sample(frac= 0.2, replace = False, random_state=1)\n",
    "\n",
    "\n",
    "range_n_clusters = [2,3,4,5,6,7]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(tiny_pca) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, \n",
    "                       init = 'k-means++',\n",
    "                       random_state=1, \n",
    "                       n_jobs = -1)\n",
    "    cluster_labels = clusterer.fit_predict(tiny_pca)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(tiny_pca, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(tiny_pca, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(tiny_pca.iloc[:, 0], tiny_pca.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 4,\n",
    "            init = 'k-means++',\n",
    "            random_state=1, \n",
    "            n_jobs = -1).fit(tiny_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Significant Findings**\n",
    "\n",
    "- The silhouette scores provided additional insight into the validity of our clustering strategy. While 2 clusters provide the highest cluster validity, the significant imbalance of the clusters led us to choose a less optimal clustering in exchange for gains in the group membership balance. Rather than the 5 cluster arrangement, we elect to proceed with a 4 cluster arrangement because of its higher silhouette score.\n",
    "\n",
    "- The combination of PCA and K-means led to the establishment of 4 clusters with an acceptable cluster validity based on an average silhouette score of .57. \n",
    "\n",
    "- A major limitation if this strategy is the relative densities of the clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE<a class=\"anchor\" id=\"t-SNE\"></a>\n",
    "\n",
    "A limitation of using only the Principal Components on our dataset is that the Euclidean distance between the first two components varied widely across the dataset, making an accurate interpretation of the clustering difficult at best. To solve this problem, we will use a variant of Stochastic Neighborhood Embedding (t-SNE). This technique solves the density problem making adjustments to the distributions of the input variables and distribute the vectors more evenly across each dimension.\n",
    "\n",
    "Methodology:\n",
    "- Our input will be the reduced X_pca dataset that consists of the 88 principal components that represent 99% of the variance in the dataset.\n",
    "- We must down-sample this dataset by 80% to reduce the time-complexity of the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsampling X_pca\n",
    "dfX_pca = pd.DataFrame(X_pca)\n",
    "tiny_pca = dfX_pca.sample(frac = .2, replace =False)\n",
    "tiny_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_res = dict()\n",
    "for i in range(5, 55, 5):\n",
    "    tsne = TSNE(n_components = 2, random_state = 42, perplexity = i, n_jobs =-1)\n",
    "    tsne_res[i] = tsne.fit_transform(tiny_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5, figsize = (50,10));\n",
    "fig.suptitle('Perplexity Plots');\n",
    "\n",
    "for i, key in enumerate(tsne_res.keys()):\n",
    "    if i % 2 == 0:\n",
    "        ax[0, i // 2].scatter(\n",
    "            tsne_res[key][:, 0],\n",
    "            tsne_res[key][:, 1],\n",
    "            s = 0.5\n",
    "        )\n",
    "    else:\n",
    "        ax[1, (i - 1) // 2].scatter(\n",
    "            tsne_res[key][:, 0],\n",
    "            tsne_res[key][:, 1],\n",
    "            s = 0.5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As a benchmark, we will visualize where the t-SNE method dispurses the k_means labeled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_res[50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components = 2, random_state=random_state, perplexity = 50, n_jobs = -1)\n",
    "tsne_comp = tsne.fit_transform(tiny_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_clusters(tsne, labels, s = 0.5, figsize = (10,10)):\n",
    "    \"\"\"Plot tsne 2D composition\n",
    "    \"\"\"\n",
    "    num_classes = len(np.unique(labels))\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.scatter(tsne[:, 0], tsne[:, 1], c = palette[labels], s = s);\n",
    "plt.subplot(1,2,1)\n",
    "tsne_clusters(tsne_comp, km.labels_)\n",
    "plt.subplot(1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing DBSCAN to take advantage of the variance in the densities\n",
    "\n",
    "# lets first look at the connectivity of the graphs and distance to the nearest neighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "#=======================================================\n",
    "# CHANGE THESE VALUES TO ADJUST MINPTS FOR EACH DATASET\n",
    "minpts= 2500\n",
    "\n",
    "#=======================================================\n",
    "    # create connectivity graphs before calcualting the hierarchy\n",
    "knn_graph = kneighbors_graph(tsne_res[50], \n",
    "                             minpts,\n",
    "                             mode='distance',\n",
    "                             n_jobs = -1) # calculate distance to four nearest neighbors \n",
    "    \n",
    "\n",
    "N2 = knn_graph.shape[0]\n",
    "nn_distances = np.zeros((N2,1))\n",
    "for i in range(N2):\n",
    "    nn_distances[i] = knn_graph[i,:].max()\n",
    "\n",
    "nn_distances = np.sort(nn_distances, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(N2), nn_distances, 'r.', markersize=2) #plot the data\n",
    "plt.title('Dataset name: tsne_res(50), sorted by neighbor distance')\n",
    "plt.xlabel('X2, Instance Number')\n",
    "plt.ylabel('X2, Distance to {0}th nearest neighbor'.format(minpts))\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#=====================================\n",
    "# ENTER YOUR CODE HERE TO CHANGE MINPTS AND EPS FOR EACH DATASET\n",
    "minpts = [2100,2500]\n",
    "eps = [6.55, 7.7]\n",
    "#=====================================\n",
    "\n",
    "for e,m in zip(eps, minpts):\n",
    "\n",
    "    db = DBSCAN(eps=e, \n",
    "                min_samples=m,\n",
    "                leaf_size = 41,\n",
    "                n_jobs = -1,\n",
    "                metric = 'euclidean',\n",
    "               ).fit(tsne_res[50])\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    # mark the samples that are considered \"core\"\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "    plt.figure(figsize=(15,8))\n",
    "    unique_labels = set(labels) # the unique labels\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = 'k'\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = tsne_res[50][class_member_mask & core_samples_mask]\n",
    "        # plot the core points in this class\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "                 markeredgecolor='w', markersize=6)\n",
    "\n",
    "        # plot the remaining points that are edge points\n",
    "        xy = tsne_res[50][class_member_mask & ~core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "                 markeredgecolor='w', markersize=3)\n",
    "\n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Option B: Association Rule Mining (Q3B)<a class=\"anchor\" id=\"Rule_mining\"></a>\n",
    "\n",
    "Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Option C: Collaborative Filtering (Q3C)<a class=\"anchor\" id=\"Collaborative\"></a>\n",
    "\n",
    "Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stage Five - Deployment (Q3) <a class=\"anchor\" id=\"Deployment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Next Stage Deployment <a class=\"anchor\" id=\"Deployment\"></a>\n",
    "\n",
    "The next stage in the CRISP-DM is deployment. After model building and evaluation, we are ready to deploy our code representation of the model into a production environment and solve our original business problem.\n",
    "Our business problem is to give Home Credit loan evaluators access to a model that evaluates an applicant’s current and past financial history in determining whether to approve the requested loan.\n",
    "\n",
    "#### How useful is your model for interested parties?<a class=\"anchor\" id=\"Useful\"></a>\n",
    "\n",
    "We believe this model would be useful for loan departments loan evaluators. This is contingent of discovering how some of the external variables are created. With this and some additional consistancy in top scoring features we could improve the accuracy\n",
    "\n",
    "#### How would you measure the model's value if it was used by these parties?<a class=\"anchor\" id=\"Value\"></a>\n",
    "\n",
    "This model should be tested in parrellel to present evaluation proceess. Then after a set period of time compare human evaluation to model based accuracy. If the results were the same, the minimum resulting savings would be the salaries of the loan evaluators. Added value would result from the acceleration of the loan approval process.\n",
    "\n",
    "#### How would your deploy your model for interested parties?<a class=\"anchor\" id=\"Deploy\"></a>\n",
    "\n",
    "Depending on the resources,  available, models can be deployed as batch or real-time predictions. Home Credits current process is a batch implementation. The applicant fills out the form which is then digitized and sent to the loan approval department. During the loan approval, the collected data will need to be cleaned and normalized before processed through the machine learning predictive model.\n",
    "\n",
    "#### How often would the model need to be updated?<a class=\"anchor\" id=\"Update\"></a>\n",
    "\n",
    "The metrics for the customer is continually updated. On a scheduled cycle, this new dataset is analyzed through the current model. Doing batch cycles will allow for consistency in whether an applicant is approved or not.  However, model designs will change as newer technology are available or changing business environments make reengineering required.\n",
    "\n",
    "#### What other data should be collected?<a class=\"anchor\" id=\"Collect\"></a>\n",
    "\n",
    "In addition to the clarification of data we have described above, there would be a tremendous value in the financials for the industry that the individual is working. These financials could assist in prediction of any future concerns for the applicants present income.\n",
    "\n",
    "Below is a typical example of a batch deployment \\[1\\].\n",
    "\n",
    "<img src=\"../_images/batchdeployment_process.png\" style=\"width:800px;height:375px\"/>\n",
    "\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "\\[1\\] J. Kervizic, Overview of Different Approaches to Deploying Machine Learning Models in Production, June 2016.\n",
    "Accessed on: Feb. 15, 2019. \\[Online\\].\n",
    "Available: https://www.kdnuggets.com/2019/06/approaches-deploying-machine-learning-production.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1. Parameter Tuning <a class=\"anchor\" id=\"A2\"></a>\n",
    "\n",
    "The code and output of model tuneing in shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
