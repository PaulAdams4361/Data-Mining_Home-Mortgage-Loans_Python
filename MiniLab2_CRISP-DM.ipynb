{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Schwan, Stuart Miller, Justin Howard, Paul Adams\n",
    "# Lab Two: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniLab2 Project Requirments -\n",
    "1. [Data Preparation](#DataPreparation)\n",
    "    1. [Define and prepare your class variables](#Define)\n",
    "    2. [Describe the final dataset that is used for classification/regression (include a\n",
    "description of any newly formed variables you created](#FinalDataset)\n",
    "\n",
    "2. [Modeling and Evaluation](#DataUnderstanding)\n",
    "    1. [Choose and explain your evaluation metrics that you will use](#Evaluaation)\n",
    "    2. [Choose the method you will use for dividing your data into training and\n",
    "testing splits](#DivideData)\n",
    "    3. [Create three different classification/regression models](#Models)\n",
    "    4. [Analyze the results using your chosen method of evaluation](#Analyze)\n",
    "    5. [Discuss the advantages of each model for each classification task](#Advantages)\n",
    "    6. [Which attributes from your analysis are most important](#Attributes)\n",
    "    \n",
    "3. [Deployment](#Deployment)\n",
    "    1. [How useful is your model for interested parties (i.e., the companies or\n",
    "organizations that might want to use it for prediction)? How would you measure the\n",
    "model's value if it was used by these parties? How would your deploy your model for\n",
    "interested parties? What other data should be collected? How often would the model\n",
    "need to be updated, etc.?](#Value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are to build upon the predictive analysis (classification) that you already completed in the\n",
    "previous mini-project, adding additional modeling from new classification algorithms as well as\n",
    "more explanations that are inline with the CRISP-DM framework. You should use appropriate cross\n",
    "validation for all of your analysis (explain your chosen method of performance validation in detail).\n",
    "Try to use as much testing data as possible in a realistic manner (you should define what you think\n",
    "is realistic and why).\n",
    "This report is worth 20% of the final grade. Please upload a report (one per team) with all code\n",
    "used, visualizations, and text in a single document. The format of the document can be PDF,\n",
    "*.ipynb, or HTML. You can write the report in whatever format you like, but it is easiest to turn in the\n",
    "rendered iPython notebook. The results should be reproducible using your report. Please carefully\n",
    "describe every assumption and every step in your report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./crisps-dm2.png\" style=\"width:550px;height:450px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Selection\n",
    "Select a dataset identically to the way you selected for the first project work week and mini-project.\n",
    "You are not required to use the same dataset that you used in the past, but you are encouraged.\n",
    "You must identify two tasks from the dataset to regress or classify. That is:\n",
    "• two classification tasks OR\n",
    "• two regression tasks OR\n",
    "• one classification task and one regression task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Stage Three - Data Preperation (Q1)   <a class=\"anchor\" id=\"DataPreparation\"></a>\n",
    "We will use the Home Credit Default Risk dataset made available on Kaggle to develop a useful model that predicts loan defaults for a majority of the loan applicants whose population is defined by the given training and test datasets. Predicting loan defaults is essential to the profitability of banks and, given the competitive nature of the loan market, a bank that collects the right data can offer and service more loans. This analysis of Home Credit's Default Risk dataset will focus on generating accurate loan default risk probabilities, identifying sub-populations among the given applicants, and finally, the most critical factors that indicate that an applicant will likely default on their loan.\n",
    "\n",
    "FROM TEMPLATE\n",
    "This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your data mining goals, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define and prepare your class variables (Q1A)<a class=\"anchor\" id=\"Define\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define and prepare your class variables. Use proper variable\n",
    "representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for\n",
    "dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for\n",
    "the analysis.**\n",
    "\n",
    "Home Credit is an international non-bank financial institution that operates in 10 countries and focuses on lending to people with little or no credit history. This institution has served 11 million customers, is based in the Czechia, and is a significant consumer lender in most of the Commonwealth of Independent States Countries, especially Russia. Recently, it has established a presence in China and the United States. The dataset provided is extensive, representing 307,511 applications from various locations. \n",
    "\n",
    "The data types vary in scale and type, from time-series credit histories to demographic indicators. Our analysis will focus on two datasets, data collected in the application train and test datasets, and several engineered features gathered from the millions of credit bureau records for each loan applicant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:/users/howar/documents/machine_learning1/home-credit-default-risk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6c7b0f221e70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c:/users/howar/documents/machine_learning1/home-credit-default-risk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Import Libraries Required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'c:/users/howar/documents/machine_learning1/home-credit-default-risk'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('c:/users/howar/documents/machine_learning1/home-credit-default-risk')\n",
    "# Import Libraries Required.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "#removing warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data that was preprocessed using the custom read_clean_data() function, \n",
    "# merged with the previously engineered newFeatures from Lab 1 \n",
    "\n",
    "data = pd.read_csv('data.csv', na_values = 'XNA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.CNT_FAM_MEMBERS.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains features, such as the number of children an applicant has, where the number of unique values are relatively high, but the percentage of the total makeup is almost imperceptable. The distributions of these high cardinality variables are heavily skewed. To address this issue, the number of numeric categories was reduced to get a more accurate interpretation of their impact on the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recoding high cardinality features\n",
    "\n",
    "# reducing the child count feature to 3 categories\n",
    "def cnt_child(series):\n",
    "    if series == 0 :\n",
    "        return 'No Children'\n",
    "    elif 1 <= series < 5 :\n",
    "        return '1-4 Children'\n",
    "    else :\n",
    "        return '5 or More Children'\n",
    "data['cnt_child'] = data.CNT_CHILDREN.apply(cnt_child).astype('category')\n",
    "\n",
    "# reducing family count feature to 4 categories\n",
    "def cnt_family(series):\n",
    "    if series == 1:\n",
    "        return '1 Family Member'\n",
    "    elif series == 2: \n",
    "        return '2 Family Members'\n",
    "    elif 3 >= series <= 5:\n",
    "        return '3 - -5 Family Members'\n",
    "    else :\n",
    "        return '6 or more Family Members'\n",
    "data['cnt_family'] = data.CNT_FAM_MEMBERS.apply(cnt_family).astype('category')\n",
    "\n",
    "# reducing engineered feature CREDIT_ACTIVE to 4 categories\n",
    "data.CREDIT_ACTIVE = data.CREDIT_ACTIVE.astype(np.uint32)\n",
    "\n",
    "def credit_active(series):\n",
    "    if series == 0:\n",
    "        return 'No Accounts'\n",
    "    elif 1 <= series <= 3:\n",
    "        return '1-3 Accounts'\n",
    "    else : \n",
    "        return ' > 4 Accounts'\n",
    "data['credit_active'] = data.CREDIT_ACTIVE.apply(credit_active).astype('category')\n",
    "\n",
    "# reducing engineered feature LOAN_COUNT to 5 categories\n",
    "\n",
    "def loan_count(series):\n",
    "    if series == 0:\n",
    "        return 'No Loans'\n",
    "    elif 1 <= series <= 2:\n",
    "        return '1-2 Loans'\n",
    "    elif 3 <= series <= 5:\n",
    "        return '3-5 Loans'\n",
    "    elif 6 <= series <= 10:\n",
    "        return '6-10 Loans'\n",
    "    else : \n",
    "        return ' > 10 Loans'\n",
    "\n",
    "data['loan_cnt'] = data.LOAN_COUNT.apply(loan_count).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.2 Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). (Q1b) <a class=\"anchor\" id=\"FinalDataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping variables that were deemed the least useful for analysis\n",
    "new_df = data.copy().drop(['TARGET', 'SK_ID_CURR', 'AMT_GOODS_PRICE', 'CNT_CHILDREN',\n",
    "                          'CNT_FAM_MEMBERS', 'EXT_SOURCE_1_AV','EXT_SOURCE_2_AV', 'EXT_SOURCE_3_AV', 'LOAN_COUNT',\n",
    "                          'CREDIT_ACTIVE'], axis = 1)\n",
    "\n",
    "# adding drop_first = True eliminates the duplication of categorical features that are already binary indicators\n",
    "new_df = pd.get_dummies(new_df, drop_first = True)\n",
    "pd.options.display.max_columns = 400\n",
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(new_df)\n",
    "X_train_std = sc.transform(new_df) \n",
    "X_train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(X_train_std.T)\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "print('\\nTop Ten Eigenvalues \\n%s' % eigen_vals[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the total var explained and cumulative variance\n",
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i/tot) for i in sorted(eigen_vals, reverse =True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "len(var_exp)\n",
    "len(cum_var_exp)\n",
    "print(\"Variance Explained length: \" + str(len(var_exp)), \"\\nCumulative Variance Explained length : \" + str(len(cum_var_exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(1,191), var_exp, alpha = .5, align = 'center', label = 'Individual Explained Variance')\n",
    "\n",
    "plt.step(range(1,191), cum_var_exp, where = 'mid', label = 'Cumulative Variance Explained')\n",
    "\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Componenet Index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture at least 80% of the variability in the dataset, we must include at least 100 principal components. We will pair the eigenvectors with their corresponding eigenvalues and project them onto a 2 dimensional subspace and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a listof (eigenvalues, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "\n",
    "#sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "\"\"\"eigen_pairs[0] is equivalent to the single eigenvalue for PC1 and the eigenvector that represents the 190 features of the data\"\"\"\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse = True)\n",
    "\n",
    "#collecting the two eigenvectors that correspond to the two largest eigenvalues\n",
    "\n",
    "W = np.hstack((eigen_pairs[0][1][:,np.newaxis],\n",
    "               eigen_pairs[1][1][:,np.newaxis]))\n",
    "\n",
    "# printing the first 5 pairs\n",
    "print('Matrix W: \\n', W[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this matrix to transform the training set into new features and plot them. First, we will observe the first 2 Principal Components with a logistic regression-based decision boundary to view the discriminatory ability of a logistic model using the Principal Components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper script\n",
    "from plot_decision_regions import plot_decision_regions\n",
    "# other packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#transforming training data\n",
    "X_train_pca = X_train_std.dot(W)\n",
    "\n",
    "# identifying labels\n",
    "y_train = data.TARGET\n",
    "\n",
    "#initializing the PCA transformer and logistic regression estimator:\n",
    "pca = PCA(n_components =2)\n",
    "lr = LogisticRegression(multi_class = 'ovr',\n",
    "                        random_state = 1,\n",
    "                        solver = 'lbfgs')\n",
    "# dimensionality reduction:\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "#fitting the logitistic regression model on the reduced dataset:\n",
    "lr.fit(X_train_pca, y_train)\n",
    "plot_decision_regions(X_train_pca, y_train, classifier = lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we can see that class separation is very poor and non-linear. We have two centroids where each has only a slight concentric-ellipse type of separation between the two classes. The decision boundary drawn by a logistic model is clearly unable to use the first two principal compoenents to discriminate between defaults and non-defaulted loans.\n",
    "\n",
    "We can also attempt to view the value of the third principal component to see if there is good separation when a third dimension is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting three principal components\n",
    "W3 = np.hstack((W,eigen_pairs[2][1][:,np.newaxis]))\n",
    "W3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming training data\n",
    "X_train_pca3 = X_train_std.dot(W3)\n",
    "X_train_pca3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-d plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "\n",
    "colors = ['r','g']\n",
    "markers = ['o', 'x']\n",
    "\n",
    "# for each index and class in:\n",
    "for idx, cl in enumerate(np.unique(y_train)):\n",
    "    ax.scatter3D(X_train_pca3[y_train == cl, 0],\n",
    "                X_train_pca3[y_train == cl, 1],\n",
    "                X_train_pca3[y_train == cl, 2],\n",
    "                label = cl,  \n",
    "                c = X_train_pca3[y_train == cl, 2],\n",
    "                cmap = 'viridis',\n",
    "                marker = markers[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A three dimensional plot of the data does provide some insights. Defaults can be separated along the third dimension, as indicated by the defaults being clustered in the blue region and the non-defaults being clustered in the green/yellow regions  of the third Principal Component. The overlap between the classes is so signficant that the boundary line is not clear.\n",
    "\n",
    "To solve this problem, we will apply sampling strategies that will clarify the class boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Stage Four - Modeling and Evaluation (Q2) <a class=\"DataUnderstanding\" id=\"EDA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Choose and explain your evaluation metrics (Q2A) <a class=\"anchor\" id=\"Evaluaation\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the significant overlap between the classes in our dataset, we have elected to evaluate our models using the recall metric. Recall is the ratio of accurately predicted defaults to total defaults, which allows us to extract value from the model. \n",
    "\n",
    "We will provide an example of why we deemed this necessary.\n",
    "\n",
    "Below is a Gradient Boosting Machines model that uses the entire dataset to make class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # enabling sklearn's experimental gradient boosting machine algorithm\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    " # now you can import normally from ensemble\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "clf = HistGradientBoostingClassifier().fit(X_train_std, y_train)\n",
    "clf.score(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 92% is deceptive. The significant imbalance of the dataset creates this deception. The goal of the analysis is to build a model capable of predicting loan defaults. If we remember that over 90% of the loans in the dataset were in good standing, we can then understand how our model can easily achieve an accuracy of at least 90% by sheer coincidence. \n",
    "\n",
    "We will now observe the recall of the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf, X_train_std, y_train, cv = 5, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall ability of this model ranges from 1.1% to 1.3%. This reveals that the model is useless for predicting loan defaults. \n",
    "\n",
    "We will experiment with various resampling strategies until significant improvements are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Choose the method you will use for dividing your data (Q2B) <a class=\"anchor\" id=\"Describedata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using an 80/20 training : test set split.\n",
    "\n",
    "As previously mentioned in section 1.2 (Data Description) we concluded that it was necessary to implement a sampling strategy on the training set in order to make a useful model for predicting loan defaults. \n",
    "\n",
    "As a justification of the use of a generic sampling strategy with this dataset, we will provide an example of the effectiveness of simple Randum Under Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X = X_train_std\n",
    "y = data.TARGET\n",
    "rus = RandomUnderSampler(random_state = 1)\n",
    "X_rus, y_rus = rus.fit_resample(X,y)\n",
    "y_rus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HistGradientBoostingClassifier().fit(X_rus, y_rus)\n",
    "clf.score(X_rus, y_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_rus, y_rus, cv = 5, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying random under-sampling of the majority class permits the gradient boosting algorithm to better define the class differences. Our ability to detect loans that enter default status increased over 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  Create three different classification/regression models (Q2C) <a class=\"anchor\" id=\"Models\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Analyze the results using your chosen method of evaluation (Q2D) <a class=\"anchor\" id=\"Analyze\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Discuss the advantages of each model for each classification task (Q2E) <a class=\"anchor\" id=\"Advantages\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Which attributes from your analysis are most important (Q2F) <a class=\"anchor\" id=\"Attributes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stage Five - Deployment (Q3) <a class=\"anchor\" id=\"Deployment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? (Q3A) <a class=\"anchor\" id=\"Value\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
